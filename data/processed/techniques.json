{
  "categories": [
    {
      "id": "basic-concepts",
      "name": "Basic Concepts",
      "description": "Fundamental prompting structures and conceptual frameworks",
      "techniques": [
        {
          "id": "basic-prompting",
          "name": "Basic Prompting",
          "aliases": [
            "Standard Prompting",
            "Vanilla Prompting"
          ],
          "description": "The simplest form of prompting, usually consisting of an instruction and input, without exemplars or complex reasoning steps.",
          "sources": [
            "Vatsal & Dubey",
            "Schulhoff et al.",
            "Wei et al."
          ],
          "relatedTechniques": [
            "instructed-prompting",
            "zero-shot-learning"
          ],
          "useCase": "Simple, direct tasks where clarity is paramount. Effective for well-defined tasks with clear instructions.",
          "example": "Translate the following English text to French: 'Hello, how are you?'",
          "tips": "Be specific and clear in your instructions. Avoid ambiguous language. Include context when necessary. State the desired output format explicitly.",
          "commonMistakes": "Being too vague or general. Not providing enough context. Assuming the model knows unstated requirements. Using complex language when simple is better."
        },
        {
          "id": "few-shot-learning",
          "name": "Few-Shot Learning/Prompting",
          "description": "Providing K > 1 demonstrations in the prompt to help the model understand patterns.",
          "sources": [
            "Brown et al.",
            "Wei et al.",
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "one-shot-learning",
            "zero-shot-learning",
            "in-context-learning"
          ],
          "useCase": "Tasks where examples help illustrate the desired pattern or format of response.",
          "example": "Classify the sentiment of the following restaurant reviews as positive or negative:\n\nExample 1: 'The food was delicious.' Sentiment: positive\nExample 2: 'Terrible service and cold food.' Sentiment: negative\n\nNew review: 'The atmosphere was nice but waiting time was too long.'",
          "tips": "Choose diverse, high-quality examples. Ensure examples clearly demonstrate the pattern. Use 2-5 examples for best results. Keep examples concise but complete.",
          "commonMistakes": "Using poor-quality or inconsistent examples. Too many examples that confuse rather than clarify. Examples that don't match the actual task."
        },
        {
          "id": "zero-shot-learning",
          "name": "Zero-Shot Learning/Prompting",
          "description": "Prompting with instruction only, without any demonstrations or examples.",
          "sources": [
            "Brown et al.",
            "Vatsal & Dubey",
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "few-shot-learning",
            "one-shot-learning",
            "instructed-prompting"
          ],
          "useCase": "Simple tasks or when working with capable models that don't require examples.",
          "example": "Summarize the main points of the following article in 3 bullet points: [article text]",
          "tips": "Make instructions as clear and specific as possible. Include output format requirements. Consider the model's capabilities and limitations.",
          "commonMistakes": "Underestimating task complexity. Not providing sufficient context. Expecting perfect results without examples for complex tasks."
        },
        {
          "id": "one-shot-learning",
          "name": "One-Shot Learning/Prompting",
          "description": "Providing exactly one demonstration in the prompt to help the model understand patterns.",
          "sources": [
            "Brown et al.",
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "few-shot-learning",
            "zero-shot-learning",
            "in-context-learning"
          ],
          "useCase": "When a single example sufficiently conveys the pattern or when context length is limited.",
          "example": "Translate English to French:\nEnglish: The weather is beautiful today.\nFrench: Le temps est beau aujourd'hui.\n\nEnglish: I would like to order dinner."
        },
        {
          "id": "in-context-learning",
          "name": "In-Context Learning (ICL)",
          "description": "The model's ability to learn from demonstrations/instructions within the prompt at inference time, without updating weights.",
          "sources": [
            "Brown et al.",
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "few-shot-learning",
            "exemplar-selection",
            "exemplar-ordering"
          ],
          "useCase": "Achieving task-specific behavior without fine-tuning, particularly effective for classification, translation, and reasoning tasks.",
          "example": "Q: What is the capital of France?\nA: Paris\n\nQ: What is the capital of Japan?\nA: Tokyo\n\nQ: What is the capital of Australia?\nA:"
        },
        {
          "id": "cloze-prompts",
          "name": "Cloze Prompts",
          "description": "Prompts with masked slots for prediction, often in the middle of the text.",
          "sources": [
            "Wang et al. - Healthcare Survey",
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "prefix-prompts",
            "fill-in-the-blank-format"
          ],
          "useCase": "Extractive QA, knowledge probing, and logical completion tasks.",
          "example": "The capital of France is _____."
        },
        {
          "id": "prefix-prompts",
          "name": "Prefix Prompts",
          "description": "Standard prompt format where the prediction follows the input.",
          "sources": [
            "Wang et al. - Healthcare Survey",
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "cloze-prompts",
            "continuous-prompt"
          ],
          "useCase": "Most general-purpose prompting scenarios where text completion is desired.",
          "example": "Write a short poem about autumn:"
        },
        {
          "id": "template-prompting",
          "name": "Templating (Prompting)",
          "description": "Using functions with variable slots to construct prompts in a systematic way.",
          "sources": [
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "basic-prompting",
            "instruction-selection"
          ],
          "useCase": "When standardizing prompts across multiple inputs or creating programmatic interfaces.",
          "example": "def generate_summary_prompt(text):\n    return f\"Summarize the following text in 3 sentences:\\n\\n{text}\""
        },
        {
          "id": "instructed-prompting",
          "name": "Instructed Prompting",
          "description": "Explicitly instructing the LLM with clear directions about the task.",
          "sources": [
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "basic-prompting",
            "zero-shot-learning"
          ],
          "useCase": "Any task where specific behavioral guidance is needed.",
          "example": "You are a professional translator. Translate the following English text to Spanish, maintaining the same tone and formality level:"
        },
        {
          "id": "role-prompting",
          "name": "Role Prompting",
          "description": "Assigning a specific role or persona to the model.",
          "sources": [
            "Nori et al."
          ],
          "relatedTechniques": [
            "instructed-prompting"
          ],
          "useCase": "Tasks requiring domain expertise or specific tone/style.",
          "example": "You are an experienced tax accountant with expertise in small business taxation. Help me understand the tax implications of..."
        }
      ]
    },
    {
      "id": "reasoning-frameworks",
      "name": "Reasoning Frameworks",
      "description": "Techniques that guide the model through explicit reasoning steps",
      "techniques": [
        {
          "id": "chain-of-thought",
          "name": "Chain-of-Thought (CoT) Prompting",
          "description": "Eliciting step-by-step reasoning before the final answer, usually via few-shot exemplars.",
          "sources": [
            "Wei et al.",
            "Schulhoff et al.",
            "Vatsal & Dubey",
            "Wang et al. - Self-Consistency"
          ],
          "relatedTechniques": [
            "zero-shot-cot",
            "few-shot-cot",
            "self-consistency"
          ],
          "useCase": "Complex reasoning tasks, math problems, logical deductions, and multi-step decision processes.",
          "example": "Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nLet's think about this step-by-step:\n1. Roger starts with 5 tennis balls\n2. He buys 2 cans of tennis balls, with 3 balls per can\n3. So he gets 2 × 3 = 6 new tennis balls\n4. In total, he has 5 + 6 = 11 tennis balls\n\nAnswer: 11 tennis balls"
        },
        {
          "id": "zero-shot-cot",
          "name": "Zero-Shot CoT",
          "description": "Appending a thought-inducing phrase without CoT exemplars, like 'Let's think step by step'.",
          "sources": [
            "Schulhoff et al.",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "few-shot-cot"
          ],
          "useCase": "When example chains of reasoning aren't available but step-by-step thinking is still beneficial.",
          "example": "Question: If a store has 10 apples and 3 people each buy 2 apples, how many apples are left?\n\nLet's think step by step."
        },
        {
          "id": "few-shot-cot",
          "name": "Few-Shot CoT",
          "description": "CoT prompting using multiple CoT exemplars to demonstrate the reasoning process.",
          "sources": [
            "Schulhoff et al.",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "zero-shot-cot"
          ],
          "useCase": "Complex reasoning tasks where the model needs to learn specific reasoning patterns.",
          "example": "Q: Roger has 5 tennis balls. He buys 2 cans, each with 3 tennis balls. How many tennis balls does he have now?\nA: Roger starts with 5 tennis balls. He buys 2 cans, each with 3 tennis balls. So he gets 2×3=6 more tennis balls. In total, he has 5+6=11 tennis balls.\n\nQ: Alice has 7 books. She gives 2 books to Bob and buys 3 more books. How many books does she have now?"
        },
        {
          "id": "tree-of-thoughts",
          "name": "Tree-of-Thoughts (ToT)",
          "description": "Exploring multiple reasoning paths in a tree structure using generate, evaluate, and search methods.",
          "sources": [
            "Yao et al.",
            "Vatsal & Dubey",
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "graph-of-thoughts",
            "self-consistency"
          ],
          "useCase": "Complex problems with multiple possible approaches, where exploring alternatives is beneficial.",
          "example": "Problem: Find the optimal strategy for the game of 24 (reach 24 using +, -, *, / with cards 3, 9, 4, 1).\n\nPath 1: (3 + 9) * (4 - 1) = 12 * 3 = 36 (invalid)\nPath 2: (3 * 9 - 4) - 1 = 27 - 4 - 1 = 22 (invalid)\nPath 3: (3 + 1) * 9 - 4 = 4 * 9 - 4 = 36 - 4 = 32 (invalid)\nPath 4: 3 * (9 - 1) - 4 = 3 * 8 - 4 = 24 - 4 = 20 (invalid)\nPath 5: (9 - 1) * (4 - 3) = 8 * 1 = 8 (invalid)\nPath 6: 3 * 9 - 4 - 1 = 27 - 4 - 1 = 22 (invalid)\nPath 7: 3 * (9 - 4) + 1 = 3 * 5 + 1 = 15 + 1 = 16 (invalid)\nPath 8: (3 + 9) * 4 / (1 + 3) = 12 * 4 / 4 = 12 (invalid)\nPath 9: 9 * 4 / 3 + 1 = 36 / 3 + 1 = 12 + 1 = 13 (invalid)\nPath 10: (9 - 1) * 3 = 8 * 3 = 24 (valid!)"
        },
        {
          "id": "skeleton-of-thought",
          "name": "Skeleton-of-Thought (SoT)",
          "description": "A two-stage approach: first generating a skeleton (outline) and then expanding points in parallel.",
          "sources": [
            "Ning et al.",
            "Schulhoff et al."
          ],
          "relatedTechniques": [
            "tree-of-thoughts",
            "parallel-point-expanding"
          ],
          "useCase": "Long-form content generation where structure is important, like essays or reports.",
          "example": "Task: Write an essay about climate change.\n\nSkeleton:\n1. Introduction to climate change\n2. Causes of climate change\n3. Effects on ecosystems\n4. Economic impacts\n5. Potential solutions\n6. Conclusion\n\n[Then each point is expanded in parallel]"
        },
        {
          "id": "graph-of-thoughts",
          "name": "Graph-of-Thoughts (GoT)",
          "description": "Extending Tree-of-Thoughts with more flexible graph structures for complex reasoning.",
          "sources": [
            "Besta et al."
          ],
          "relatedTechniques": [
            "tree-of-thoughts",
            "chain-of-thought"
          ],
          "useCase": "Complex reasoning tasks requiring non-linear thought processes and cyclic dependencies.",
          "example": "Problem solving that involves feedback loops and interconnected reasoning paths."
        },
        {
          "id": "least-to-most-prompting",
          "name": "Least-to-Most Prompting",
          "description": "Breaking down complex problems into simpler subproblems and solving them sequentially.",
          "sources": [
            "Zhou et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "step-back-prompting"
          ],
          "useCase": "Complex compositional reasoning tasks that can be decomposed into simpler parts.",
          "example": "To solve this complex problem, let's break it down: What are the simpler subproblems? Let's solve them step by step."
        },
        {
          "id": "recursion-of-thought",
          "name": "Recursion-of-Thought (RoT)",
          "description": "Using recursive problem-solving approaches in prompting.",
          "sources": [
            "Zhao et al."
          ],
          "relatedTechniques": [
            "least-to-most-prompting",
            "tree-of-thoughts"
          ],
          "useCase": "Problems that naturally decompose into similar subproblems.",
          "example": "Apply the same reasoning pattern recursively to solve this problem."
        },
        {
          "id": "plan-and-solve-prompting",
          "name": "Plan-and-Solve Prompting",
          "description": "First devising a plan to solve the problem, then executing the plan step by step.",
          "sources": [
            "Wang et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "least-to-most-prompting"
          ],
          "useCase": "Multi-step problems requiring strategic planning before execution.",
          "example": "Let's devise a plan to solve this problem: 1) First, let's understand what we need to find. 2) Then, let's identify the steps needed."
        },
        {
          "id": "step-back-prompting",
          "name": "Step-Back Prompting",
          "description": "Taking a step back to ask higher-level questions before solving specific problems.",
          "sources": [
            "Zheng et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "least-to-most-prompting"
          ],
          "useCase": "Problems requiring understanding of broader principles before specific implementation.",
          "example": "Before solving this specific problem, let's step back: What are the general principles that apply here?"
        },
        {
          "id": "program-of-thoughts",
          "name": "Program-of-Thoughts (PoT)",
          "description": "Expressing reasoning as executable programs rather than natural language.",
          "sources": [
            "Chen et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "code-based-agents"
          ],
          "useCase": "Mathematical and computational reasoning tasks.",
          "example": "Let's solve this step by step using code: ```python\n# Step 1: Define variables\n# Step 2: Apply operations\n```"
        },
        {
          "id": "maieutic-prompting",
          "name": "Maieutic Prompting",
          "description": "Using a question-driven approach to guide reasoning through self-questioning.",
          "sources": [
            "Jung et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "self-ask"
          ],
          "useCase": "Complex reasoning tasks where questioning assumptions is important.",
          "example": "To understand this problem, let's ask ourselves: What do we know? What don't we know? What assumptions are we making?"
        },
        {
          "id": "chain-of-verification",
          "name": "Chain-of-Verification (CoVe)",
          "description": "Generating initial responses, then creating and answering verification questions to improve accuracy.",
          "sources": [
            "Dhuliawala et al."
          ],
          "relatedTechniques": [
            "self-verification",
            "chain-of-thought"
          ],
          "useCase": "Tasks requiring high accuracy where initial responses need verification.",
          "example": "Initial answer: [response]. Now let's verify: What questions should I ask to check this answer? Let me answer these verification questions."
        }
      ]
    },
    {
      "id": "agent-tool-use",
      "name": "Agent & Tool Use",
      "description": "Techniques that enable LLMs to interact with external tools and environments",
      "techniques": [
        {
          "id": "agent-based-prompting",
          "name": "Agent-Based Prompting",
          "description": "Assigning an agent role to the LLM that can use tools, make decisions, and interact with the environment.",
          "sources": [
            "Park et al.",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "react",
            "tool-use-agents"
          ],
          "useCase": "Complex tasks requiring tool use, decision making, and multi-step reasoning.",
          "example": "You are a research agent with access to a search tool. To use the tool, format your response as [SEARCH(query)]."
        },
        {
          "id": "react",
          "name": "ReAct (Reasoning + Acting)",
          "description": "Combining reasoning traces and task-specific actions in an interleaved manner.",
          "sources": [
            "Yao et al.",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "agent-based-prompting",
            "chain-of-thought"
          ],
          "useCase": "Tasks requiring both reasoning and interaction with external tools/environments.",
          "example": "Thought: I need to find when the Golden Gate Bridge was built. Action: Search(Golden Gate Bridge construction date)"
        },
        {
          "id": "mrkl-system",
          "name": "MRKL System",
          "description": "Modular Reasoning, Knowledge and Language system combining neural language models with symbolic tools.",
          "sources": [
            "Karpas et al."
          ],
          "relatedTechniques": [
            "tool-use-agents",
            "pal"
          ],
          "useCase": "Tasks requiring both neural reasoning and symbolic computation.",
          "example": "Combining language understanding with calculator, search, and other symbolic tools."
        },
        {
          "id": "pal",
          "name": "Program-Aided Language Models (PAL)",
          "description": "Reading natural language problems and generating programs as intermediate reasoning steps.",
          "sources": [
            "Gao et al."
          ],
          "relatedTechniques": [
            "program-of-thoughts",
            "code-based-agents"
          ],
          "useCase": "Mathematical and logical reasoning tasks that benefit from programmatic solutions.",
          "example": "Let me solve this by writing a program: ```python\ndef solve_problem():\n    # reasoning as code\n```"
        },
        {
          "id": "critic",
          "name": "CRITIC",
          "description": "Correcting with Retrieval and Iterative Tool Interaction and Critique.",
          "sources": [
            "Gou et al."
          ],
          "relatedTechniques": [
            "self-correction",
            "tool-use-agents"
          ],
          "useCase": "Tasks requiring iterative improvement through tool use and self-critique.",
          "example": "Generate initial answer, critique it using tools, then refine the response."
        },
        {
          "id": "taskweaver",
          "name": "TaskWeaver",
          "description": "A code-first agent framework for seamlessly planning and executing data analytics tasks.",
          "sources": [
            "Qiao et al."
          ],
          "relatedTechniques": [
            "code-based-agents",
            "tool-use-agents"
          ],
          "useCase": "Data analytics and computational tasks requiring code generation and execution.",
          "example": "Planning and executing data analysis workflows through code generation."
        },
        {
          "id": "tool-use-agents",
          "name": "Tool-Use Agents",
          "description": "Agents specifically designed to interact with and use external tools effectively.",
          "sources": [
            "Qin et al.",
            "Schick et al."
          ],
          "relatedTechniques": [
            "agent-based-prompting",
            "react"
          ],
          "useCase": "Tasks requiring access to external knowledge, computation, or services.",
          "example": "You can use these tools: Search(query), Calculator(expression), Weather(location). Use them when needed."
        },
        {
          "id": "code-based-agents",
          "name": "Code-Based Agents",
          "description": "Agents that primarily operate through code generation and execution.",
          "sources": [
            "Hong et al."
          ],
          "relatedTechniques": [
            "program-of-thoughts",
            "pal"
          ],
          "useCase": "Programming tasks, data analysis, and computational problem solving.",
          "example": "Solving problems by writing and executing code snippets."
        },
        {
          "id": "gitm",
          "name": "Generate, Implement, Test, and Modify (GITM)",
          "description": "An iterative framework for code generation involving generation, implementation, testing, and modification.",
          "sources": [
            "Chen et al."
          ],
          "relatedTechniques": [
            "code-based-agents",
            "self-correction"
          ],
          "useCase": "Software development tasks requiring iterative refinement.",
          "example": "1) Generate code, 2) Implement it, 3) Test for errors, 4) Modify based on results."
        },
        {
          "id": "reflexion",
          "name": "Reflexion",
          "description": "Learning from self-reflection and environmental feedback to improve performance on subsequent attempts.",
          "sources": [
            "Shinn et al."
          ],
          "relatedTechniques": [
            "self-correction",
            "agent-based-prompting"
          ],
          "useCase": "Tasks where learning from failures and iterative improvement is beneficial.",
          "example": "Attempt task, reflect on failures, incorporate lessons learned, and try again."
        },
        {
          "id": "voyager",
          "name": "Voyager",
          "description": "A lifelong learning agent with a growing skill library for open-ended exploration.",
          "sources": [
            "Wang et al."
          ],
          "relatedTechniques": [
            "agent-based-prompting",
            "code-based-agents"
          ],
          "useCase": "Open-ended exploration and skill acquisition in complex environments.",
          "example": "Building a library of reusable skills for continuous learning and exploration."
        },
        {
          "id": "tora",
          "name": "ToRA (Tool-integrated Reasoning Agent)",
          "description": "Integrating multiple tools into reasoning processes for mathematical problem solving.",
          "sources": [
            "Gou et al."
          ],
          "relatedTechniques": [
            "tool-use-agents",
            "program-of-thoughts"
          ],
          "useCase": "Mathematical reasoning tasks requiring computational tools.",
          "example": "Solving complex math problems by integrating reasoning with computational tools."
        }
      ]
    },
    {
      "id": "self-improvement",
      "name": "Self-Improvement Techniques",
      "description": "Methods for the model to reflect on and improve its own outputs",
      "techniques": [
        {
          "id": "self-consistency",
          "name": "Self-Consistency",
          "description": "Generating multiple reasoning paths and selecting the most consistent answer.",
          "sources": [
            "Wang et al. - Self-Consistency",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "self-verification"
          ],
          "useCase": "Complex reasoning tasks where multiple approaches might yield different answers.",
          "example": "Problem: What is 17 × 36? Path 1: ... Path 2: ... Consistent Answer: 612"
        },
        {
          "id": "self-correction",
          "name": "Self-Correction",
          "description": "Model reviews and revises its own output.",
          "sources": [
            "Madaan et al.",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "self-critique",
            "self-evaluation"
          ],
          "useCase": "Error reduction, iterative improvement.",
          "example": "After generating your answer, review it for any errors or issues, and provide a corrected version."
        },
        {
          "id": "self-refine",
          "name": "Self-Refine",
          "description": "Iteratively refining outputs through self-feedback without additional training.",
          "sources": [
            "Madaan et al."
          ],
          "relatedTechniques": [
            "self-correction",
            "self-critique"
          ],
          "useCase": "Improving output quality through iterative refinement.",
          "example": "Generate initial output, provide self-feedback, then refine based on that feedback."
        },
        {
          "id": "self-verification",
          "name": "Self-Verification",
          "description": "Having the model verify the correctness of its own answers.",
          "sources": [
            "Manakul et al.",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "self-consistency"
          ],
          "useCase": "Tasks where verifying results is critical.",
          "example": "Answer: The derivative of f(x) = x² is f'(x) = 2x. Verification: ..."
        },
        {
          "id": "self-calibration",
          "name": "Self-Calibration",
          "description": "Adjusting confidence estimates to better match actual accuracy.",
          "sources": [
            "Kadavath et al."
          ],
          "relatedTechniques": [
            "self-verification"
          ],
          "useCase": "Tasks requiring accurate confidence estimates.",
          "example": "Provide answer with confidence level: Answer: [response] (Confidence: 85%)"
        },
        {
          "id": "reverse-chain-of-thought",
          "name": "Reverse Chain-of-Thought",
          "description": "Working backwards from conclusions to verify reasoning paths.",
          "sources": [
            "Xue et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "self-verification"
          ],
          "useCase": "Verifying logical reasoning by working backwards from conclusions.",
          "example": "Given this conclusion, let's work backwards to verify the reasoning path."
        },
        {
          "id": "self-ask",
          "name": "Self-Ask",
          "description": "Model asks itself follow-up questions to improve reasoning.",
          "sources": [
            "Press et al."
          ],
          "relatedTechniques": [
            "maieutic-prompting",
            "chain-of-thought"
          ],
          "useCase": "Complex reasoning tasks requiring clarification of intermediate steps.",
          "example": "Initial question: [question]. Follow-up: Do I need to know [sub-question] to answer this?"
        },
        {
          "id": "universal-self-consistency",
          "name": "Universal Self-Consistency",
          "description": "Applying self-consistency across different reasoning formats and approaches.",
          "sources": [
            "Chen et al."
          ],
          "relatedTechniques": [
            "self-consistency",
            "chain-of-thought"
          ],
          "useCase": "Complex reasoning requiring consistency across multiple approaches.",
          "example": "Solve using multiple methods (algebraic, geometric, computational) and check for consistency."
        },
        {
          "id": "metacognitive-prompting",
          "name": "Metacognitive Prompting",
          "description": "Encouraging the model to think about its own thinking processes.",
          "sources": [
            "Velez et al."
          ],
          "relatedTechniques": [
            "self-ask",
            "self-verification"
          ],
          "useCase": "Complex problem-solving requiring awareness of reasoning strategies.",
          "example": "Before solving, ask: What strategy should I use? How confident am I? What could go wrong?"
        },
        {
          "id": "self-generated-icl",
          "name": "Self-Generated In-Context Learning",
          "description": "Model generates its own examples for in-context learning.",
          "sources": [
            "Kim et al."
          ],
          "relatedTechniques": [
            "in-context-learning",
            "few-shot-learning"
          ],
          "useCase": "Tasks where relevant examples are not readily available.",
          "example": "First, generate relevant examples for this task, then use them to solve the problem."
        }
      ]
    },
    {
      "id": "retrieval-augmentation",
      "name": "Retrieval & Augmentation",
      "description": "Techniques that incorporate external knowledge into prompts",
      "techniques": [
        {
          "id": "rag",
          "name": "Retrieval-Augmented Generation (RAG)",
          "description": "Enhancing LLM responses by retrieving relevant information from external sources.",
          "sources": [
            "Lewis et al.",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "dsp"
          ],
          "useCase": "Tasks requiring specific factual information beyond the model's training data.",
          "example": "Question: What were the key provisions of the Paris Climate Agreement? [System retrieves relevant documents...]"
        },
        {
          "id": "dsp",
          "name": "Demonstration-Search-Predict (DSP)",
          "description": "A retrieval technique that searches for demonstrations relevant to the input query.",
          "sources": [
            "Khattab et al.",
            "Vatsal & Dubey"
          ],
          "relatedTechniques": [
            "rag"
          ],
          "useCase": "Tasks benefiting from retrieving similar examples.",
          "example": "Question: How does photosynthesis work? [System searches for relevant demonstrations...]"
        },
        {
          "id": "iterative-retrieval-augmentation",
          "name": "Iterative Retrieval Augmentation",
          "description": "Multiple rounds of retrieval and generation for complex tasks.",
          "sources": [
            "Trivedi et al."
          ],
          "relatedTechniques": [
            "rag",
            "self-ask"
          ],
          "useCase": "Complex questions requiring multiple pieces of information.",
          "example": "Retrieve initial information, generate partial response, identify gaps, retrieve more information."
        },
        {
          "id": "interleaved-retrieval-guided-cot",
          "name": "Interleaved Retrieval-Guided Chain-of-Thought",
          "description": "Combining retrieval with chain-of-thought reasoning in an interleaved manner.",
          "sources": [
            "Trivedi et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "rag"
          ],
          "useCase": "Complex reasoning tasks requiring external knowledge at multiple steps.",
          "example": "Step 1: Reason about what info is needed. Retrieve. Step 2: Use info to reason further..."
        },
        {
          "id": "implicit-rag",
          "name": "Implicit RAG",
          "description": "Retrieval-augmented generation where the retrieval process is implicit and automatic.",
          "sources": [
            "Zhou et al."
          ],
          "relatedTechniques": [
            "rag"
          ],
          "useCase": "Seamless knowledge augmentation without explicit retrieval steps.",
          "example": "System automatically retrieves relevant information without explicit retrieval commands."
        },
        {
          "id": "verify-and-edit",
          "name": "Verify-and-Edit",
          "description": "Using retrieval to verify and correct generated content.",
          "sources": [
            "Zhao et al."
          ],
          "relatedTechniques": [
            "self-verification",
            "rag"
          ],
          "useCase": "Fact-checking and improving accuracy of generated content.",
          "example": "Generate initial response, retrieve facts to verify, edit based on retrieved information."
        },
        {
          "id": "cross-file-code-completion-prompting",
          "name": "Cross-File Code Completion Prompting",
          "description": "Using information from multiple files for code completion and generation.",
          "sources": [
            "Ding et al."
          ],
          "relatedTechniques": [
            "retrieved-cross-file-context"
          ],
          "useCase": "Code generation tasks requiring context from multiple files.",
          "example": "Complete this function considering the context from related files in the codebase."
        },
        {
          "id": "retrieved-cross-file-context",
          "name": "Retrieved Cross-File Context",
          "description": "Retrieving relevant context from multiple files to inform code generation.",
          "sources": [
            "Zhang et al."
          ],
          "relatedTechniques": [
            "cross-file-code-completion-prompting"
          ],
          "useCase": "Code understanding and generation in large codebases.",
          "example": "Retrieving function definitions, imports, and usage patterns from related files."
        }
      ]
    },
    {
      "id": "prompt-optimization",
      "name": "Prompt Optimization",
      "description": "Techniques to automate and improve prompt engineering",
      "techniques": [
        {
          "id": "automated-prompt-optimization",
          "name": "Automated Prompt Optimization",
          "description": "Using algorithms to automatically improve prompt effectiveness.",
          "sources": [
            "Zhou et al."
          ],
          "relatedTechniques": [
            "ape",
            "grips"
          ],
          "useCase": "Optimizing prompts for specific tasks or datasets.",
          "example": "Automatically testing and refining prompt variations to maximize performance."
        },
        {
          "id": "ape",
          "name": "Automatic Prompt Engineer (APE)",
          "description": "Automatically generates and optimizes prompts for a given task.",
          "sources": [
            "Zhou et al."
          ],
          "relatedTechniques": [
            "grips"
          ],
          "useCase": "Automating prompt design for large-scale or complex tasks.",
          "example": "Given a task, APE generates multiple candidate prompts and selects the best-performing one."
        },
        {
          "id": "grips",
          "name": "GRIPS",
          "description": "Gradient-based prompt search for optimization.",
          "sources": [
            "Prasad et al."
          ],
          "relatedTechniques": [
            "ape"
          ],
          "useCase": "Optimizing prompts using gradient-based methods.",
          "example": "GRIPS iteratively updates prompt tokens to maximize task performance."
        },
        {
          "id": "continuous-prompt-optimization",
          "name": "Continuous Prompt Optimization",
          "description": "Optimizing prompts in continuous vector spaces rather than discrete text.",
          "sources": [
            "Li & Liang"
          ],
          "relatedTechniques": [
            "soft-prompt-tuning"
          ],
          "useCase": "Fine-grained prompt optimization using continuous representations.",
          "example": "Optimizing prompt embeddings in continuous space for better performance."
        },
        {
          "id": "discrete-prompt-optimization",
          "name": "Discrete Prompt Optimization",
          "description": "Optimizing prompts at the discrete token level.",
          "sources": [
            "Shin et al."
          ],
          "relatedTechniques": [
            "ape",
            "automated-prompt-optimization"
          ],
          "useCase": "Finding optimal discrete prompt tokens for specific tasks.",
          "example": "Searching through discrete token spaces to find optimal prompt formulations."
        },
        {
          "id": "hybrid-prompt-optimization",
          "name": "Hybrid Prompt Optimization",
          "description": "Combining continuous and discrete optimization approaches for prompts.",
          "sources": [
            "Qin & Eisner"
          ],
          "relatedTechniques": [
            "continuous-prompt-optimization",
            "discrete-prompt-optimization"
          ],
          "useCase": "Leveraging benefits of both continuous and discrete optimization.",
          "example": "Using continuous optimization for exploration and discrete optimization for final prompts."
        },
        {
          "id": "soft-prompt-tuning",
          "name": "Soft Prompt Tuning",
          "description": "Learning continuous prompt embeddings while keeping the model frozen.",
          "sources": [
            "Lester et al."
          ],
          "relatedTechniques": [
            "continuous-prompt-optimization"
          ],
          "useCase": "Task-specific adaptation without modifying model parameters.",
          "example": "Learning task-specific prompt embeddings that are prepended to input."
        },
        {
          "id": "rlprompt",
          "name": "RLPrompt",
          "description": "Using reinforcement learning to optimize prompts based on task performance.",
          "sources": [
            "Deng et al."
          ],
          "relatedTechniques": [
            "automated-prompt-optimization"
          ],
          "useCase": "Optimizing prompts using reward signals from task performance.",
          "example": "Training an RL agent to generate prompts that maximize task-specific rewards."
        },
        {
          "id": "fm-based-optimization",
          "name": "Foundation Model-Based Optimization",
          "description": "Using large language models themselves to optimize prompts.",
          "sources": [
            "Yang et al."
          ],
          "relatedTechniques": [
            "ape"
          ],
          "useCase": "Leveraging language model capabilities for prompt improvement.",
          "example": "Using an LLM to critique and improve existing prompts iteratively."
        },
        {
          "id": "genetic-algorithm-optimization",
          "name": "Genetic Algorithm Optimization",
          "description": "Applying genetic algorithms to evolve better prompts.",
          "sources": [
            "Meyerson et al."
          ],
          "relatedTechniques": [
            "automated-prompt-optimization"
          ],
          "useCase": "Evolving prompts through mutation and selection processes.",
          "example": "Creating populations of prompts, mutating them, and selecting the best performers."
        },
        {
          "id": "gradient-based-optimization",
          "name": "Gradient-Based Optimization",
          "description": "Using gradient information to optimize prompt effectiveness.",
          "sources": [
            "Wen et al."
          ],
          "relatedTechniques": [
            "grips",
            "continuous-prompt-optimization"
          ],
          "useCase": "Leveraging gradient information for efficient prompt optimization.",
          "example": "Computing gradients with respect to prompt parameters and updating accordingly."
        }
      ]
    },
    {
      "id": "multimodal-techniques",
      "name": "Multimodal Techniques",
      "description": "Techniques involving non-text modalities like images, audio, and video",
      "techniques": [
        {
          "id": "3d-prompting",
          "name": "3D Prompting",
          "description": "Incorporating 3D spatial information and models into prompts.",
          "sources": [
            "Liu et al."
          ],
          "relatedTechniques": [
            "multimodal-chain-of-thought"
          ],
          "useCase": "3D modeling, spatial reasoning, and architectural tasks.",
          "example": "Given this 3D model, analyze the structural integrity and suggest improvements."
        },
        {
          "id": "audio-prompting",
          "name": "Audio Prompting",
          "description": "Using audio inputs as part of the prompt context.",
          "sources": [
            "Zhang et al."
          ],
          "relatedTechniques": [
            "multimodal-in-context-learning"
          ],
          "useCase": "Speech recognition, audio analysis, and music-related tasks.",
          "example": "Listen to this audio clip and transcribe the speech, noting any emotional tone."
        },
        {
          "id": "image-prompting",
          "name": "Image Prompting",
          "description": "Incorporating images as part of the prompt to guide model outputs.",
          "sources": [
            "Tsimpoukelli et al."
          ],
          "relatedTechniques": [
            "multimodal-chain-of-thought"
          ],
          "useCase": "Tasks requiring visual context or image-based reasoning.",
          "example": "Prompt: [Image of a cat] Describe what you see."
        },
        {
          "id": "video-prompting",
          "name": "Video Prompting",
          "description": "Using video content as context for generating responses.",
          "sources": [
            "Wang et al."
          ],
          "relatedTechniques": [
            "multimodal-chain-of-thought"
          ],
          "useCase": "Video analysis, temporal reasoning, and motion understanding.",
          "example": "Analyze this video sequence and describe the sequence of events."
        },
        {
          "id": "chain-of-images",
          "name": "Chain-of-Images",
          "description": "Using sequences of images to guide reasoning processes.",
          "sources": [
            "Meng et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "image-prompting"
          ],
          "useCase": "Visual reasoning tasks requiring sequential image analysis.",
          "example": "Image 1: [Initial state] → Image 2: [Process] → Image 3: [Result]. Explain the transformation."
        },
        {
          "id": "multimodal-chain-of-thought",
          "name": "Multimodal Chain-of-Thought",
          "description": "Combining reasoning over text and images in a step-by-step manner.",
          "sources": [
            "Zhu et al."
          ],
          "relatedTechniques": [
            "image-prompting"
          ],
          "useCase": "Complex tasks involving both text and images.",
          "example": "Given an image and a question, reason step by step using both modalities."
        },
        {
          "id": "multimodal-graph-of-thought",
          "name": "Multimodal Graph-of-Thought",
          "description": "Extending graph-of-thought reasoning to multimodal inputs.",
          "sources": [
            "Chen et al."
          ],
          "relatedTechniques": [
            "graph-of-thoughts",
            "multimodal-chain-of-thought"
          ],
          "useCase": "Complex multimodal reasoning with non-linear thought processes.",
          "example": "Reasoning about relationships between images, text, and other modalities in graph form."
        },
        {
          "id": "multimodal-in-context-learning",
          "name": "Multimodal In-Context Learning",
          "description": "Learning from multimodal examples provided in context.",
          "sources": [
            "Alayrac et al."
          ],
          "relatedTechniques": [
            "in-context-learning",
            "few-shot-learning"
          ],
          "useCase": "Learning from examples that include both text and other modalities.",
          "example": "Here are examples of image-caption pairs: [examples]. Now caption this new image."
        },
        {
          "id": "image-as-text-prompting",
          "name": "Image-as-Text Prompting",
          "description": "Converting images to textual descriptions for text-based models.",
          "sources": [
            "Yang et al."
          ],
          "relatedTechniques": [
            "image-prompting"
          ],
          "useCase": "Using visual information in text-only models.",
          "example": "Convert image to description: 'A red car parked next to a blue house.' Now process this description."
        },
        {
          "id": "negative-prompting-image",
          "name": "Negative Prompting for Images",
          "description": "Specifying what should not appear in generated images.",
          "sources": [
            "Liu et al."
          ],
          "relatedTechniques": [
            "image-prompting"
          ],
          "useCase": "Image generation with specific exclusions.",
          "example": "Generate an image of a forest scene. Negative prompt: no buildings, no vehicles, no people."
        },
        {
          "id": "paired-image-prompting",
          "name": "Paired Image Prompting",
          "description": "Using pairs of related images to guide reasoning or generation.",
          "sources": [
            "Kim et al."
          ],
          "relatedTechniques": [
            "image-prompting",
            "chain-of-images"
          ],
          "useCase": "Comparative analysis and before/after reasoning tasks.",
          "example": "Compare these two images: [Image A] [Image B]. What are the key differences?"
        }
      ]
    },
    {
      "id": "specialized-application",
      "name": "Specialized Application Techniques",
      "description": "Techniques optimized for specific domains or applications",
      "techniques": [
        {
          "id": "alphacodium",
          "name": "AlphaCodeium",
          "description": "Advanced code generation system using iterative refinement and testing.",
          "sources": [
            "Ridnik et al."
          ],
          "relatedTechniques": [
            "code-generation-agents",
            "gitm"
          ],
          "useCase": "Complex programming tasks requiring iterative refinement.",
          "example": "Generate code, test it, analyze failures, and iteratively improve until passing all tests."
        },
        {
          "id": "code-generation-agents",
          "name": "Code Generation Agents",
          "description": "Agents specialized for generating and refining code.",
          "sources": [
            "Chen et al."
          ],
          "relatedTechniques": [
            "chain-of-thought"
          ],
          "useCase": "Automated code writing and debugging.",
          "example": "Write a Python function to reverse a string."
        },
        {
          "id": "scot",
          "name": "Structured Chain-of-Thought (SCoT)",
          "description": "Applying structured reasoning to specific domains like mathematics.",
          "sources": [
            "Wang et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "mathprompter"
          ],
          "useCase": "Domain-specific reasoning requiring structured approaches.",
          "example": "Solving mathematical problems with structured step-by-step reasoning."
        },
        {
          "id": "tab-cot",
          "name": "Tab-CoT",
          "description": "Chain-of-thought reasoning for tabular data analysis.",
          "sources": [
            "Jin et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "chain-of-table"
          ],
          "useCase": "Analyzing and reasoning about structured tabular data.",
          "example": "Given this table of sales data, analyze trends step by step."
        },
        {
          "id": "chain-of-table",
          "name": "Chain-of-Table",
          "description": "Structured reasoning over tabular data with explicit table operations.",
          "sources": [
            "Wang et al."
          ],
          "relatedTechniques": [
            "tab-cot",
            "chain-of-thought"
          ],
          "useCase": "Complex table analysis and manipulation tasks.",
          "example": "Step 1: Filter table by criteria. Step 2: Calculate aggregates. Step 3: Compare results."
        },
        {
          "id": "dater",
          "name": "DATER",
          "description": "Date and time reasoning for temporal question answering.",
          "sources": [
            "Zhao et al."
          ],
          "relatedTechniques": [
            "chain-of-thought"
          ],
          "useCase": "Questions involving dates, times, and temporal relationships.",
          "example": "If today is March 15, 2024, what day of the week was January 1, 2024?"
        },
        {
          "id": "logicot",
          "name": "LogiCoT",
          "description": "Logic-focused chain-of-thought for logical reasoning tasks.",
          "sources": [
            "Liu et al."
          ],
          "relatedTechniques": [
            "chain-of-thought",
            "formal-logic"
          ],
          "useCase": "Logical reasoning, theorem proving, and formal logic tasks.",
          "example": "Given these premises, use logical steps to derive the conclusion."
        },
        {
          "id": "mathprompter",
          "name": "MathPrompter",
          "description": "Prompting techniques specialized for mathematical problem solving.",
          "sources": [
            "Wang et al."
          ],
          "relatedTechniques": [
            "chain-of-thought"
          ],
          "useCase": "Solving math word problems.",
          "example": "Solve: If a train travels 60 miles in 1.5 hours, what is its average speed?"
        },
        {
          "id": "chain-of-code",
          "name": "Chain-of-Code",
          "description": "Combining natural language reasoning with code execution for problem solving.",
          "sources": [
            "Li et al."
          ],
          "relatedTechniques": [
            "program-of-thoughts",
            "code-generation-agents"
          ],
          "useCase": "Problems requiring both reasoning and computational execution.",
          "example": "Reason about the problem, then write and execute code to compute the answer."
        },
        {
          "id": "modular-code-generation",
          "name": "Modular Code Generation",
          "description": "Breaking down code generation into modular components.",
          "sources": [
            "Nijkamp et al."
          ],
          "relatedTechniques": [
            "code-generation-agents",
            "least-to-most-prompting"
          ],
          "useCase": "Complex software development requiring modular design.",
          "example": "Generate code by breaking down functionality into reusable modules."
        },
        {
          "id": "flow-engineering",
          "name": "Flow Engineering",
          "description": "Designing structured workflows for complex task completion.",
          "sources": [
            "Hong et al."
          ],
          "relatedTechniques": [
            "agent-based-prompting",
            "plan-and-solve-prompting"
          ],
          "useCase": "Complex multi-step processes requiring workflow management.",
          "example": "Design a workflow: Input → Process A → Decision Point → Process B → Output."
        },
        {
          "id": "test-based-iterative-flow",
          "name": "Test-Based Iterative Flow",
          "description": "Using testing to guide iterative improvement in workflows.",
          "sources": [
            "Zhang et al."
          ],
          "relatedTechniques": [
            "flow-engineering",
            "gitm"
          ],
          "useCase": "Development processes requiring continuous testing and improvement.",
          "example": "Implement workflow, test results, identify failures, improve, repeat."
        }
      ]
    },
    {
      "id": "multi-agent-systems",
      "name": "Multi-Agent Systems & Team Frameworks",
      "description": "Advanced techniques for organizing and coordinating multiple AI agents",
      "techniques": [
        {
          "id": "boomerang-task-delegation",
          "name": "Boomerang Task Delegation",
          "description": "A hierarchical task decomposition pattern where complex requests are broken into subtasks, delegated to specialized modes, and their results 'boomerang' back for integration.",
          "sources": [
            "Mnehmos (2024)",
            "Building Structured AI Teams"
          ],
          "relatedTechniques": [
            "mode-based-specialization",
            "task-boundary-enforcement"
          ],
          "useCase": "Complex multi-step projects requiring coordination between specialized AI agents with different capabilities.",
          "example": "Orchestrator receives 'Build a web app' → Creates subtasks → Delegates 'Design architecture' to Architect mode → Delegates 'Write code' to Code mode → Integrates results"
        },
        {
          "id": "mode-based-specialization",
          "name": "Mode-Based Agent Specialization",
          "description": "Organizing AI systems into specialized operational modes, each with distinct capabilities, roles, and system prompts optimized for specific types of tasks.",
          "sources": [
            "Mnehmos (2024)",
            "Building Structured AI Teams"
          ],
          "relatedTechniques": [
            "boomerang-task-delegation",
            "semantic-guardrails"
          ],
          "useCase": "Systems requiring diverse capabilities where different types of tasks benefit from specialized approaches and constraints.",
          "example": "Code mode: Optimized for implementation with tool permissions for file operations. Architect mode: Focused on design with restricted file access."
        },
        {
          "id": "semantic-guardrails",
          "name": "Semantic Guardrails",
          "description": "Mode-specific validation mechanisms that monitor AI outputs for semantic drift, ensuring responses align with expected behavior and role-appropriate content.",
          "sources": [
            "Mnehmos (2024)",
            "Detecting and Correcting Emergent Errors"
          ],
          "relatedTechniques": [
            "mode-based-specialization",
            "error-pattern-libraries"
          ],
          "useCase": "Production AI systems where maintaining consistent, role-appropriate behavior is critical for reliability and user trust.",
          "example": "Code mode guardrails: Check for implementation completeness, technical precision, code quality. Architect mode guardrails: Ensure structured planning, avoid direct implementation."
        },
        {
          "id": "task-boundary-enforcement",
          "name": "Task Boundary Enforcement",
          "description": "Implementing strict schemas and validation to prevent errors from propagating between tasks in multi-agent systems through immutable inputs and sanitized outputs.",
          "sources": [
            "Mnehmos (2024)",
            "Detecting and Correcting Emergent Errors"
          ],
          "relatedTechniques": [
            "boomerang-task-delegation",
            "semantic-guardrails"
          ],
          "useCase": "Complex multi-agent workflows where error containment and task isolation are essential for system stability and debugging.",
          "example": "Define JSON schemas for task inputs/outputs → Validate at task creation → Treat contextual data as immutable → Sanitize results before parent integration"
        },
        {
          "id": "error-pattern-libraries",
          "name": "Error Pattern Libraries",
          "description": "Community-maintained repositories of common AI system errors, their causes, reproduction steps, and correction strategies to enable systematic learning from failures.",
          "sources": [
            "Mnehmos (2024)",
            "Detecting and Correcting Emergent Errors"
          ],
          "relatedTechniques": [
            "semantic-guardrails"
          ],
          "useCase": "Organizations and communities running AI systems that need to systematically capture, share, and learn from operational errors and edge cases.",
          "example": "Error: 'Semantic drift in Code mode' → Cause: 'Overly general system prompt' → Reproduction: 'Ask code mode to write poetry' → Solution: 'Add technical focus guardrail'"
        },
        {
          "id": "workflow-template-prompting",
          "name": "Workflow Template Prompting (.mdc Pattern)",
          "description": "Using structured markdown templates with YAML frontmatter to create reusable, configurable AI assistant workflows that work across different AI platforms.",
          "sources": [
            "steipete/agent-rules",
            "Cursor Rules Documentation"
          ],
          "relatedTechniques": [
            "template-prompting",
            "mode-based-specialization"
          ],
          "useCase": "Standardizing AI assistant behavior across teams and projects, creating reusable workflow automation.",
          "example": "---\ndescription: Create well-structured GitHub issues\nglobs: \"*.md\"\nalwaysApply: false\n---\n# GitHub Issue Creation\nYou are tasked with creating well-structured GitHub issues...",
          "tips": "Use YAML frontmatter for configuration metadata. Make templates generic enough for reuse. Include clear step-by-step instructions.",
          "commonMistakes": "Making templates too project-specific. Not including proper error handling instructions. Forgetting to specify output formats."
        },
        {
          "id": "ai-assistant-rule-systems",
          "name": "AI Assistant Rule Systems",
          "description": "Implementing structured rule hierarchies with global and project-specific configurations to guide AI assistant behavior consistently.",
          "sources": [
            "steipete/agent-rules",
            "Claude Code Best Practices"
          ],
          "relatedTechniques": [
            "workflow-template-prompting",
            "semantic-guardrails"
          ],
          "useCase": "Enterprise AI assistant deployments requiring consistent behavior across teams and consistent quality standards.",
          "example": "Global Rule: 'Always use conventional commit format'. Project Rule: 'Include JIRA ticket number in commits'. Combined: 'feat(auth): add OAuth support [PROJ-123]'",
          "tips": "Establish clear rule hierarchy (global → project → task-specific). Use inheritance patterns. Document rule conflicts resolution.",
          "commonMistakes": "Creating conflicting rules. Not providing examples. Making rules too rigid for edge cases."
        },
        {
          "id": "automated-development-workflows",
          "name": "Automated Development Workflows",
          "description": "Structured prompting patterns for common development tasks like commits, PR reviews, issue analysis, and code quality checks.",
          "sources": [
            "steipete/agent-rules",
            "vincenthopf/claude-code"
          ],
          "relatedTechniques": [
            "workflow-template-prompting",
            "chain-of-thought"
          ],
          "useCase": "Standardizing development processes through AI assistance, ensuring consistent quality and documentation.",
          "example": "Commit Workflow: 1) Analyze changes 2) Categorize by conventional commit type 3) Generate clear, descriptive message 4) Add relevant emoji 5) Include breaking change notes if needed",
          "tips": "Break complex workflows into clear steps. Include validation criteria. Provide fallback options for edge cases.",
          "commonMistakes": "Skipping validation steps. Not handling merge conflicts. Assuming perfect git state."
        },
        {
          "id": "mcp-server-integration-patterns",
          "name": "MCP Server Integration Patterns",
          "description": "Prompting techniques for integrating and orchestrating Model Context Protocol servers to extend AI capabilities with external tools and services.",
          "sources": [
            "steipete/agent-rules MCP Best Practices",
            "Model Context Protocol Documentation"
          ],
          "relatedTechniques": [
            "tool-use-agents",
            "react"
          ],
          "useCase": "Building sophisticated AI systems that integrate with external APIs, databases, and services through standardized protocols.",
          "example": "Tool Integration: 'Use the github MCP server to create an issue: {\"title\": \"Bug report\", \"body\": \"Detailed description\", \"labels\": [\"bug\"]}' → Server executes → Return results",
          "tips": "Design for sensible defaults. Implement proper error handling. Use file-based logging. Validate configurations.",
          "commonMistakes": "Not handling server failures gracefully. Ignoring stdio output restrictions. Poor error messages."
        },
        {
          "id": "github-integration-prompting",
          "name": "GitHub Integration Prompting",
          "description": "Structured approaches for AI assistants to interact with GitHub repositories, issues, PRs, and project management through systematic research and action patterns.",
          "sources": [
            "steipete/agent-rules",
            "@nityeshaga GitHub Best Practices"
          ],
          "relatedTechniques": [
            "automated-development-workflows",
            "mcp-server-integration-patterns"
          ],
          "useCase": "Automating GitHub workflow tasks like issue creation, PR reviews, and project management through AI assistance.",
          "example": "Issue Creation Flow: 1) Research repository conventions 2) Analyze similar issues 3) Structure according to templates 4) Generate comprehensive description 5) Execute 'gh issue create' with proper metadata",
          "tips": "Always research repository conventions first. Use templates when available. Include proper labeling and assignment.",
          "commonMistakes": "Not researching existing conventions. Creating duplicate issues. Poor categorization."
        },
        {
          "id": "agent-configuration-management",
          "name": "Agent Configuration Management",
          "description": "Systematic approaches to managing AI agent configurations, including global settings, project-specific rules, and environment-specific adaptations.",
          "sources": [
            "steipete/agent-rules",
            "Commanding Your Claude Code Army"
          ],
          "relatedTechniques": [
            "ai-assistant-rule-systems",
            "mode-based-specialization"
          ],
          "useCase": "Managing complex AI assistant deployments across multiple projects, teams, and environments with consistent behavior.",
          "example": "Configuration Hierarchy: ~/.claude/CLAUDE.md (global) → .cursor/rules/*.mdc (project) → task-specific prompts → runtime context",
          "tips": "Use hierarchical configuration systems. Implement configuration validation. Provide clear inheritance rules.",
          "commonMistakes": "Configuration conflicts. No validation. Poor documentation of precedence rules."
        }
      ]
    }
  ]
}